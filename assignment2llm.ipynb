{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11294512,"sourceType":"datasetVersion","datasetId":7062259},{"sourceId":182534,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":155593,"modelId":178059}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:08:26.120681Z","iopub.execute_input":"2025-04-07T16:08:26.121155Z","iopub.status.idle":"2025-04-07T16:08:26.157852Z","shell.execute_reply.started":"2025-04-07T16:08:26.121122Z","shell.execute_reply":"2025-04-07T16:08:26.156819Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llmdataset/glove.6B.300d.txt\n/kaggle/input/llmdataset/data.jsonl\n/kaggle/input/distillbert/transformers/default/1/config.json\n/kaggle/input/distillbert/transformers/default/1/tokenizer.json\n/kaggle/input/distillbert/transformers/default/1/tokenizer_config.json\n/kaggle/input/distillbert/transformers/default/1/model.safetensors\n/kaggle/input/distillbert/transformers/default/1/special_tokens_map.json\n/kaggle/input/distillbert/transformers/default/1/vocab.txt\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"<h1>Using LSTM</h1>","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# Path to the uploaded JSONL file\nfile_path = \"/kaggle/input/llmdataset/data.jsonl\"\n\n# Read the JSONL file and collect all text entries\nall_texts = []\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        try:\n            data = json.loads(line)  # Parse JSON object from line\n            for key, value in data.items():\n                if isinstance(value, str):  # Ensure the value is a string\n                    all_texts.append(value)\n        except json.JSONDecodeError:\n            continue\n\n# Convert extracted text into a DataFrame for better visualization\ndf_texts = pd.DataFrame(all_texts, columns=[\"Extracted Text\"])\ndf_texts\n\nslovenian_texts = []\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        try:\n            data = json.loads(line.strip())\n            if data.get(\"language\") == \"Slovenian\":\n                slovenian_texts.append(data.get(\"text\", \"\"))\n        except json.JSONDecodeError:\n            continue\n\n# Display the extracted Slovenian text\ndf_slovenian = pd.DataFrame({\"Slovenian Texts\": slovenian_texts})\ndf_slovenian\n\nenglish_texts = []\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        try:\n            data = json.loads(line.strip())\n            if data.get(\"language\") == \"English\":\n                english_texts.append(data.get(\"text\", \"\"))\n        except json.JSONDecodeError:\n            continue\n\n# Display the extracted English text\ndf_english = pd.DataFrame({\"English Texts\": english_texts})\ndf_english\n\n\nimport json\nimport pandas as pd\n\ntexts = []\nlabels = []\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as file:\n    for line in file:\n        try:\n            data = json.loads(line.strip())\n            if data.get(\"language\") == \"English\":\n                texts.append(data.get(\"text\", \"\"))\n                labels.append(data.get(\"labels\", \"\"))\n        except json.JSONDecodeError:\n            continue\n\n# Create a DataFrame\ndf = pd.DataFrame({\"text\": texts, \"label\": labels})\nprint(df['label'].value_counts())  # Optional: See class distribution\n\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Hyperparameters\nmax_vocab = 10000\nmax_len = 300\n\n# Tokenize\ntokenizer = Tokenizer(num_words=max_vocab, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['text'])\nsequences = tokenizer.texts_to_sequences(df['text'])\npadded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n\n# Label encoding\nle = LabelEncoder()\nlabels = le.fit_transform(df['label'])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42, stratify=labels)\n\n# Load GloVe embeddings (download if needed)\nembedding_index = {}\nwith open(\"/kaggle/input/llmdataset/glove.6B.300d.txt\", encoding=\"utf8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coeffs = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = coeffs\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:43:17.469423Z","iopub.execute_input":"2025-04-07T17:43:17.469775Z","iopub.status.idle":"2025-04-07T17:43:53.822480Z","shell.execute_reply.started":"2025-04-07T17:43:17.469749Z","shell.execute_reply":"2025-04-07T17:43:53.821544Z"}},"outputs":[{"name":"stdout","text":"label\nInformation/Explanation    223\nNews                       211\nInstruction                184\nOpinion/Argumentation      163\nPromotion                  162\nForum                      109\nProse/Lyrical              103\nLegal                       54\nOther                       28\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"embedding_dim = 300\nword_index = tokenizer.word_index\nembedding_matrix = np.zeros((max_vocab, embedding_dim))\n\n\nfor word, i in word_index.items():\n    if i < max_vocab:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None and embedding_vector.shape == (embedding_dim,):\n            embedding_matrix[i] = embedding_vector\n\n\nfor word, i in word_index.items():\n    if i < max_vocab:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            if embedding_vector.shape != (embedding_dim,):\n                print(f\"Skipping word: {word} with shape: {embedding_vector.shape}\")\n                continue\n            embedding_matrix[i] = embedding_vector\n\n\n\nfor word, i in word_index.items():\n    if i < max_vocab:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n\nmodel = Sequential([\n    Embedding(input_dim=max_vocab, output_dim=embedding_dim, weights=[embedding_matrix],\n              input_length=max_len, trainable=False),\n    LSTM(128, return_sequences=False),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(len(le.classes_), activation='softmax')\n])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=30,\n    batch_size=32\n)\n\n\nfrom sklearn.metrics import classification_report\n\ny_pred = np.argmax(model.predict(X_test), axis=1)\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:44:15.517084Z","iopub.execute_input":"2025-04-07T17:44:15.517360Z","iopub.status.idle":"2025-04-07T17:44:37.832504Z","shell.execute_reply.started":"2025-04-07T17:44:15.517338Z","shell.execute_reply":"2025-04-07T17:44:37.831556Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                â”‚ ?                           â”‚       \u001b[38;5;34m3,000,000\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                â”‚ ?                           â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.1290 - loss: 2.1877 - val_accuracy: 0.1970 - val_loss: 2.0984\nEpoch 2/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2016 - loss: 2.1052 - val_accuracy: 0.2172 - val_loss: 2.0567\nEpoch 3/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2288 - loss: 2.0220 - val_accuracy: 0.2273 - val_loss: 1.9734\nEpoch 4/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2575 - loss: 1.9160 - val_accuracy: 0.2374 - val_loss: 2.0176\nEpoch 5/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.2581 - loss: 1.9296 - val_accuracy: 0.2727 - val_loss: 1.9220\nEpoch 6/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3007 - loss: 1.8018 - val_accuracy: 0.2626 - val_loss: 1.9413\nEpoch 7/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3751 - loss: 1.7514 - val_accuracy: 0.2828 - val_loss: 1.9074\nEpoch 8/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3658 - loss: 1.7046 - val_accuracy: 0.2828 - val_loss: 1.9000\nEpoch 9/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3897 - loss: 1.5878 - val_accuracy: 0.2879 - val_loss: 1.9148\nEpoch 10/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4492 - loss: 1.5584 - val_accuracy: 0.2828 - val_loss: 1.9514\nEpoch 11/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3302 - loss: 1.7168 - val_accuracy: 0.2828 - val_loss: 1.8561\nEpoch 12/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3540 - loss: 1.7823 - val_accuracy: 0.2879 - val_loss: 1.9072\nEpoch 13/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3732 - loss: 1.6439 - val_accuracy: 0.3333 - val_loss: 1.8360\nEpoch 14/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4192 - loss: 1.5788 - val_accuracy: 0.2475 - val_loss: 1.9369\nEpoch 15/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4263 - loss: 1.5092 - val_accuracy: 0.3081 - val_loss: 1.9036\nEpoch 16/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4897 - loss: 1.4045 - val_accuracy: 0.3283 - val_loss: 1.8529\nEpoch 17/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4673 - loss: 1.4260 - val_accuracy: 0.3384 - val_loss: 1.8678\nEpoch 18/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4967 - loss: 1.3385 - val_accuracy: 0.3283 - val_loss: 1.8513\nEpoch 19/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5201 - loss: 1.2926 - val_accuracy: 0.3232 - val_loss: 1.8843\nEpoch 20/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5596 - loss: 1.1789 - val_accuracy: 0.3182 - val_loss: 1.9393\nEpoch 21/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5128 - loss: 1.2049 - val_accuracy: 0.3283 - val_loss: 1.9779\nEpoch 22/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5899 - loss: 1.1361 - val_accuracy: 0.3081 - val_loss: 2.0447\nEpoch 23/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6241 - loss: 1.1031 - val_accuracy: 0.2677 - val_loss: 2.1816\nEpoch 24/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5904 - loss: 1.1126 - val_accuracy: 0.2980 - val_loss: 2.0292\nEpoch 25/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6224 - loss: 1.0435 - val_accuracy: 0.2828 - val_loss: 2.0919\nEpoch 26/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5948 - loss: 1.0031 - val_accuracy: 0.3081 - val_loss: 2.0725\nEpoch 27/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6588 - loss: 0.9494 - val_accuracy: 0.2778 - val_loss: 2.1983\nEpoch 28/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6093 - loss: 1.0606 - val_accuracy: 0.3131 - val_loss: 2.1411\nEpoch 29/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6860 - loss: 0.9186 - val_accuracy: 0.3232 - val_loss: 2.1868\nEpoch 30/30\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6992 - loss: 0.8661 - val_accuracy: 0.3081 - val_loss: 2.2933\n\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n                         precision    recall  f1-score   support\n\n                  Forum       0.31      0.23      0.26        22\nInformation/Explanation       0.32      0.56      0.41        45\n            Instruction       0.54      0.35      0.43        37\n                  Legal       0.82      0.82      0.82        11\n                   News       0.39      0.36      0.38        42\n  Opinion/Argumentation       0.20      0.09      0.13        33\n                  Other       0.00      0.00      0.00         5\n              Promotion       0.16      0.22      0.18        32\n          Prose/Lyrical       0.67      0.67      0.67        21\n\n               accuracy                           0.37       248\n              macro avg       0.38      0.37      0.36       248\n           weighted avg       0.37      0.37      0.36       248\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Assume df is your DataFrame containing 'text' and 'label'\n\n# Hyperparameters\nmax_vocab = 10000\nmax_len = 300\nembedding_dim = 300\n\n# Tokenize and pad sequences\ntokenizer = Tokenizer(num_words=max_vocab, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df['text'])\nsequences = tokenizer.texts_to_sequences(df['text'])\npadded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n\n# Encode labels with LabelEncoder\nle = LabelEncoder()\nlabels = le.fit_transform(df['label'])\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42, stratify=labels)\n\n# Load GloVe embeddings (300d)\nembedding_index = {}\nwith open(\"/kaggle/input/llmdataset/glove.6B.300d.txt\", encoding=\"utf8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coeffs = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = coeffs\n\n# Initialize embedding matrix\nembedding_matrix = np.zeros((max_vocab, embedding_dim))\n\nfor word, i in tokenizer.word_index.items():\n    if i < max_vocab:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None and embedding_vector.shape == (embedding_dim,):\n            embedding_matrix[i] = embedding_vector\n\n# Define the improved LSTM model\nmodel = Sequential([\n    Embedding(input_dim=max_vocab, output_dim=embedding_dim, weights=[embedding_matrix],\n              input_length=max_len, trainable=False),\n    Bidirectional(LSTM(128, return_sequences=True)),\n    BatchNormalization(),\n    Dropout(0.4),\n    Bidirectional(LSTM(64, return_sequences=False)),\n    BatchNormalization(),\n    Dropout(0.4),\n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.4),\n    Dense(len(le.classes_), activation='softmax')\n])\n\n# Compile the model\noptimizer = Adam(learning_rate=0.0005)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n# Early stopping callback\nearly_stop = EarlyStopping(patience=5, restore_best_weights=True)\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=40,\n    batch_size=64,\n    callbacks=[early_stop]\n)\n\n# Predict and evaluate\ny_pred = np.argmax(model.predict(X_test), axis=1)\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:45:13.323199Z","iopub.execute_input":"2025-04-07T17:45:13.323499Z","iopub.status.idle":"2025-04-07T17:46:10.683281Z","shell.execute_reply.started":"2025-04-07T17:45:13.323473Z","shell.execute_reply":"2025-04-07T17:46:10.682318Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - accuracy: 0.1043 - loss: 3.2722 - val_accuracy: 0.1768 - val_loss: 2.1733\nEpoch 2/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.2306 - loss: 2.4882 - val_accuracy: 0.2525 - val_loss: 2.1319\nEpoch 3/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.2820 - loss: 2.2177 - val_accuracy: 0.3232 - val_loss: 2.0867\nEpoch 4/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.3200 - loss: 2.0645 - val_accuracy: 0.3485 - val_loss: 2.0486\nEpoch 5/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.3638 - loss: 1.9769 - val_accuracy: 0.3333 - val_loss: 2.0047\nEpoch 6/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.4285 - loss: 1.6827 - val_accuracy: 0.3485 - val_loss: 1.9542\nEpoch 7/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.4948 - loss: 1.5131 - val_accuracy: 0.3737 - val_loss: 1.9180\nEpoch 8/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.5063 - loss: 1.5302 - val_accuracy: 0.3535 - val_loss: 1.8710\nEpoch 9/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.5271 - loss: 1.3784 - val_accuracy: 0.3889 - val_loss: 1.8168\nEpoch 10/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.6055 - loss: 1.2862 - val_accuracy: 0.3788 - val_loss: 1.7804\nEpoch 11/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.6078 - loss: 1.1407 - val_accuracy: 0.3838 - val_loss: 1.7562\nEpoch 12/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.6064 - loss: 1.1149 - val_accuracy: 0.3889 - val_loss: 1.6975\nEpoch 13/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.6319 - loss: 1.0403 - val_accuracy: 0.4040 - val_loss: 1.6835\nEpoch 14/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.6672 - loss: 0.9727 - val_accuracy: 0.4242 - val_loss: 1.6248\nEpoch 15/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7146 - loss: 0.8924 - val_accuracy: 0.3939 - val_loss: 1.7481\nEpoch 16/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7017 - loss: 0.8846 - val_accuracy: 0.3788 - val_loss: 1.6470\nEpoch 17/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.7281 - loss: 0.7686 - val_accuracy: 0.4596 - val_loss: 1.5585\nEpoch 18/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7924 - loss: 0.6641 - val_accuracy: 0.4545 - val_loss: 1.5685\nEpoch 19/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.7867 - loss: 0.6750 - val_accuracy: 0.4848 - val_loss: 1.4782\nEpoch 20/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.7903 - loss: 0.6180 - val_accuracy: 0.4646 - val_loss: 1.6041\nEpoch 21/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8158 - loss: 0.5737 - val_accuracy: 0.4596 - val_loss: 1.6549\nEpoch 22/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8367 - loss: 0.4998 - val_accuracy: 0.5101 - val_loss: 1.4953\nEpoch 23/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8382 - loss: 0.4966 - val_accuracy: 0.4949 - val_loss: 1.5564\nEpoch 24/40\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.8126 - loss: 0.5100 - val_accuracy: 0.4899 - val_loss: 1.7185\n\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step\n                         precision    recall  f1-score   support\n\n                  Forum       0.83      0.23      0.36        22\nInformation/Explanation       0.41      0.58      0.48        45\n            Instruction       0.57      0.78      0.66        37\n                  Legal       0.75      0.27      0.40        11\n                   News       0.50      0.60      0.54        42\n  Opinion/Argumentation       0.45      0.27      0.34        33\n                  Other       1.00      0.20      0.33         5\n              Promotion       0.31      0.28      0.30        32\n          Prose/Lyrical       0.78      0.86      0.82        21\n\n               accuracy                           0.50       248\n              macro avg       0.62      0.45      0.47       248\n           weighted avg       0.54      0.50      0.49       248\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h1>Using Distilled Bert</h1>","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:08:26.159130Z","iopub.execute_input":"2025-04-07T16:08:26.159402Z","iopub.status.idle":"2025-04-07T16:08:26.163334Z","shell.execute_reply.started":"2025-04-07T16:08:26.159377Z","shell.execute_reply":"2025-04-07T16:08:26.162544Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nimport json\nfrom sklearn.model_selection import train_test_split\n\n# Load and parse the JSONL data\nfile_path = \"/kaggle/input/llmdataset/data.jsonl\"\ntexts, labels = [], []\n\nwith open(file_path, \"r\") as f:\n    for line in f:\n        obj = json.loads(line)\n        texts.append(obj[\"text\"])  # assuming \"text\" key holds the input text\n        labels.append(obj[\"labels\"])\n\n# Encode labels to integers\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\nnum_classes = len(label_encoder.classes_)\n\n# Split dataset\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    texts, encoded_labels, test_size=0.2, random_state=42\n)\n\n# Load tokenizer for DistilBERT\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Tokenize the texts\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n\n# Create dataset class\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create datasets\ntrain_dataset = TextDataset(train_encodings, train_labels)\ntest_dataset = TextDataset(test_encodings, test_labels)\n\n# Load the DistilBERT model for sequence classification\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=num_classes\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:08:26.165116Z","iopub.execute_input":"2025-04-07T16:08:26.165588Z","iopub.status.idle":"2025-04-07T16:08:30.015566Z","shell.execute_reply.started":"2025-04-07T16:08:26.165543Z","shell.execute_reply":"2025-04-07T16:08:30.014885Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# Load and parse the JSONL data\nfile_path = \"/kaggle/input/llmdataset/data.jsonl\"\ntexts, labels = [], []\n\nwith open(file_path, \"r\") as f:\n    for line in f:\n        obj = json.loads(line)\n        texts.append(obj[\"text\"])  # assuming \"text\" key holds the input text\n        labels.append(obj[\"labels\"])\n\n# Encode labels to integers\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\nnum_classes = len(label_encoder.classes_)\n\n# Split dataset into train and test\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    texts, encoded_labels, test_size=0.2, random_state=42\n)\n\n# Load tokenizer for DistilBERT\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Tokenize\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n\n# Dataset class\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Prepare datasets\ntrain_dataset = TextDataset(train_encodings, train_labels)\ntest_dataset = TextDataset(test_encodings, test_labels)\n\n# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=num_classes\n)\n\n# Compute metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    return {\"accuracy\": acc, \"f1\": f1}\n\n# TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=20,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    logging_steps=10,\n    report_to=\"none\"  # disable W&B/Hub integration for Kaggle\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Train and evaluate\ntrainer.train()\n\n# Final evaluation on test set\nprint(\"\\nğŸ’¡ Final Evaluation on Test Set:\")\neval_results = trainer.evaluate()\nprint(eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:08:30.016733Z","iopub.execute_input":"2025-04-07T16:08:30.016949Z","iopub.status.idle":"2025-04-07T16:16:15.865241Z","shell.execute_reply.started":"2025-04-07T16:08:30.016931Z","shell.execute_reply":"2025-04-07T16:16:15.864425Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 07:36, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.839900</td>\n      <td>1.716118</td>\n      <td>0.456338</td>\n      <td>0.386276</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.427300</td>\n      <td>1.352786</td>\n      <td>0.591549</td>\n      <td>0.552184</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.259100</td>\n      <td>1.239436</td>\n      <td>0.600000</td>\n      <td>0.572388</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.022600</td>\n      <td>1.060243</td>\n      <td>0.656338</td>\n      <td>0.631344</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.887100</td>\n      <td>1.025774</td>\n      <td>0.653521</td>\n      <td>0.632102</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.748300</td>\n      <td>0.989626</td>\n      <td>0.656338</td>\n      <td>0.633309</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.730300</td>\n      <td>0.960106</td>\n      <td>0.667606</td>\n      <td>0.645543</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.567600</td>\n      <td>0.955246</td>\n      <td>0.661972</td>\n      <td>0.639871</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.609200</td>\n      <td>0.948816</td>\n      <td>0.664789</td>\n      <td>0.642916</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.526500</td>\n      <td>0.921339</td>\n      <td>0.661972</td>\n      <td>0.640725</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ’¡ Final Evaluation on Test Set:\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.9601057171821594, 'eval_accuracy': 0.6676056338028169, 'eval_f1': 0.6455433655994155, 'eval_runtime': 3.3007, 'eval_samples_per_second': 107.554, 'eval_steps_per_second': 3.636, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ğŸ” Save the trained model and tokenizer\nsave_directory = \"./my_finetuned_model\"\n\nmodel.save_pretrained(save_directory)\ntokenizer.save_pretrained(save_directory)\n\nprint(f\"âœ… Model and tokenizer saved to: {save_directory}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:16:15.866060Z","iopub.execute_input":"2025-04-07T16:16:15.866381Z","iopub.status.idle":"2025-04-07T16:16:16.836337Z","shell.execute_reply.started":"2025-04-07T16:16:15.866356Z","shell.execute_reply":"2025-04-07T16:16:16.835563Z"}},"outputs":[{"name":"stdout","text":"âœ… Model and tokenizer saved to: ./my_finetuned_model\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"<h1>Evaluate Using Transformers</h1>","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Get predictions\npreds_output = trainer.predict(test_dataset)\npreds = torch.argmax(torch.tensor(preds_output.predictions), axis=1)\nprint(preds)\n# Report\nprint(classification_report(test_labels, preds, target_names=label_encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:16:16.837169Z","iopub.execute_input":"2025-04-07T16:16:16.837493Z","iopub.status.idle":"2025-04-07T16:16:20.192381Z","shell.execute_reply.started":"2025-04-07T16:16:16.837445Z","shell.execute_reply":"2025-04-07T16:16:20.191733Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"tensor([7, 4, 7, 1, 1, 1, 5, 8, 4, 3, 5, 2, 5, 7, 7, 0, 3, 0, 5, 0, 4, 4, 5, 4,\n        4, 7, 0, 4, 7, 5, 1, 7, 4, 4, 4, 2, 7, 8, 4, 4, 7, 4, 2, 4, 4, 5, 7, 4,\n        0, 1, 1, 1, 7, 7, 4, 0, 7, 0, 7, 2, 1, 4, 8, 8, 5, 1, 7, 5, 7, 7, 8, 1,\n        4, 4, 2, 7, 5, 5, 3, 7, 8, 1, 4, 4, 5, 2, 1, 7, 2, 7, 8, 7, 2, 1, 5, 4,\n        2, 5, 5, 7, 4, 4, 5, 5, 2, 1, 4, 0, 2, 5, 7, 7, 5, 4, 1, 5, 7, 2, 4, 4,\n        5, 2, 8, 5, 3, 2, 7, 5, 1, 1, 1, 7, 5, 7, 5, 8, 3, 1, 4, 1, 1, 0, 5, 7,\n        5, 8, 1, 4, 4, 5, 8, 4, 4, 3, 4, 4, 7, 1, 4, 4, 7, 5, 2, 3, 1, 2, 4, 7,\n        1, 8, 2, 2, 1, 2, 4, 2, 4, 4, 5, 1, 1, 8, 5, 1, 0, 4, 7, 4, 4, 5, 1, 4,\n        2, 4, 7, 7, 4, 7, 1, 2, 5, 5, 1, 4, 2, 4, 1, 4, 1, 7, 4, 1, 5, 4, 1, 2,\n        1, 1, 0, 4, 7, 5, 4, 7, 1, 4, 5, 7, 0, 7, 4, 8, 4, 2, 0, 5, 4, 7, 7, 2,\n        2, 7, 5, 2, 7, 1, 1, 5, 5, 7, 0, 4, 1, 5, 1, 5, 2, 3, 5, 2, 2, 4, 4, 7,\n        5, 2, 4, 4, 0, 4, 5, 1, 7, 8, 4, 5, 2, 0, 5, 4, 8, 3, 7, 4, 4, 7, 8, 7,\n        8, 5, 2, 3, 2, 2, 4, 5, 1, 4, 4, 1, 5, 5, 7, 2, 1, 1, 8, 1, 5, 7, 2, 2,\n        3, 5, 4, 7, 2, 2, 7, 5, 4, 5, 2, 7, 7, 4, 4, 8, 4, 4, 8, 4, 4, 5, 7, 4,\n        7, 7, 5, 5, 5, 7, 1, 5, 1, 4, 2, 7, 1, 5, 1, 4, 1, 0, 2])\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"<h1>Performing Hyperparameter Tuning</h1>","metadata":{}},{"cell_type":"code","source":"search_space = [\n    {\"learning_rate\": 2e-5, \"batch_size\": 8, \"epochs\": 5},\n    {\"learning_rate\": 3e-5, \"batch_size\": 16, \"epochs\": 6},\n    {\"learning_rate\": 5e-5, \"batch_size\": 16, \"epochs\": 5},\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:49:42.314085Z","iopub.execute_input":"2025-04-07T17:49:42.314473Z","iopub.status.idle":"2025-04-07T17:49:42.318724Z","shell.execute_reply.started":"2025-04-07T17:49:42.314420Z","shell.execute_reply":"2025-04-07T17:49:42.317875Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"<h1>Fine Tuning the Hyperparameter</h1>","metadata":{}},{"cell_type":"code","source":"\n\n\nfrom transformers import Trainer, TrainingArguments, BertForSequenceClassification\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\nbest_f1 = 0\nbest_config = None\n\nprint(\"ğŸ” Starting hyperparameter tuning...\\n\")\n\nfor idx, config in enumerate(search_space, start=1):\n    print(f\"\\nğŸ”§ [{idx}/{len(search_space)}] Trying config: {config}\")\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results_{config['learning_rate']}_{config['batch_size']}\",\n        evaluation_strategy=\"epoch\",  # Replace with 'eval_strategy' if you're using Transformers >= 4.46\n        save_strategy=\"no\",\n        learning_rate=config[\"learning_rate\"],\n        per_device_train_batch_size=config[\"batch_size\"],\n        per_device_eval_batch_size=config[\"batch_size\"],\n        num_train_epochs=config[\"epochs\"],\n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        disable_tqdm=False,  # Show progress bars\n        report_to=\"none\"     # Disable WandB or other reporting integrations\n    )\n\n    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder.classes_))\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n    )\n\n    print(\"ğŸš€ Starting training...\")\n    trainer.train()\n\n    print(\"ğŸ§ª Evaluating on test set...\")\n    preds_output = trainer.predict(test_dataset)\n    preds = np.argmax(preds_output.predictions, axis=1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T17:49:45.290634Z","iopub.execute_input":"2025-04-07T17:49:45.290947Z","iopub.status.idle":"2025-04-07T18:13:31.005745Z","shell.execute_reply.started":"2025-04-07T17:49:45.290921Z","shell.execute_reply":"2025-04-07T18:13:31.005061Z"}},"outputs":[{"name":"stdout","text":"ğŸ” Starting hyperparameter tuning...\n\n\nğŸ”§ [1/3] Trying config: {'learning_rate': 2e-05, 'batch_size': 8, 'epochs': 5}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='445' max='445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [445/445 07:35, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.761800</td>\n      <td>1.634809</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.318500</td>\n      <td>1.183665</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.070300</td>\n      <td>1.044726</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.852600</td>\n      <td>0.943305</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.743000</td>\n      <td>0.913904</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ§ª Evaluating on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ”§ [2/3] Trying config: {'learning_rate': 3e-05, 'batch_size': 16, 'epochs': 6}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [270/270 08:33, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.703600</td>\n      <td>1.571611</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.314600</td>\n      <td>1.242313</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.069600</td>\n      <td>1.086593</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.822900</td>\n      <td>0.961077</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.661500</td>\n      <td>0.920714</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.617100</td>\n      <td>0.890929</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ§ª Evaluating on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ”§ [3/3] Trying config: {'learning_rate': 5e-05, 'batch_size': 16, 'epochs': 5}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 07:07, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.605700</td>\n      <td>1.459337</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.215300</td>\n      <td>1.153533</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.990200</td>\n      <td>1.019862</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.675900</td>\n      <td>0.964234</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.480300</td>\n      <td>0.907015</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ§ª Evaluating on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"    report = classification_report(test_labels, preds, output_dict=True)\n    macro_f1 = report['macro avg']['f1-score']\n\n    print(f\"ğŸ“Š Macro F1-score for current config: {macro_f1:.4f}\")\n\n    if macro_f1 > best_f1:\n        best_f1 = macro_f1\n        best_config = config\n        print(f\"ğŸ† New best config found with F1: {best_f1:.4f}\")\n\nprint(\"\\nâœ… Hyperparameter tuning complete!\")\nprint(f\"ğŸ” Best Config: {best_config}\")\nprint(f\"ğŸ¯ Best F1-score: {best_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:18:09.861740Z","iopub.execute_input":"2025-04-07T18:18:09.862040Z","iopub.status.idle":"2025-04-07T18:18:09.878566Z","shell.execute_reply.started":"2025-04-07T18:18:09.862018Z","shell.execute_reply":"2025-04-07T18:18:09.877420Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š Macro F1-score for current config: 0.6448\nğŸ† New best config found with F1: 0.6448\n\nâœ… Hyperparameter tuning complete!\nğŸ” Best Config: {'learning_rate': 5e-05, 'batch_size': 16, 'epochs': 5}\nğŸ¯ Best F1-score: 0.6448\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f\"./results_{config['learning_rate']}_{config['batch_size']}\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"no\",\n    learning_rate=config[\"learning_rate\"],\n    per_device_train_batch_size=config[\"batch_size\"],\n    per_device_eval_batch_size=config[\"batch_size\"],\n    num_train_epochs=config[\"epochs\"],\n    weight_decay=0.01,\n    warmup_steps=500,\n    lr_scheduler_type=\"linear\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    disable_tqdm=False,\n    report_to=\"none\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:18:14.904651Z","iopub.execute_input":"2025-04-07T18:18:14.904946Z","iopub.status.idle":"2025-04-07T18:18:14.939484Z","shell.execute_reply.started":"2025-04-07T18:18:14.904925Z","shell.execute_reply":"2025-04-07T18:18:14.938407Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# Final Output\nprint(\"Best Config:\", best_config)\nprint(\"Best Macro F1:\", best_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:18:19.292267Z","iopub.execute_input":"2025-04-07T18:18:19.292592Z","iopub.status.idle":"2025-04-07T18:18:19.297667Z","shell.execute_reply.started":"2025-04-07T18:18:19.292564Z","shell.execute_reply":"2025-04-07T18:18:19.296917Z"}},"outputs":[{"name":"stdout","text":"Best Config: {'learning_rate': 5e-05, 'batch_size': 16, 'epochs': 5}\nBest Macro F1: 0.6447910112962048\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"from accelerate.state import AcceleratorState\n\nfor idx, config in enumerate(search_space, start=1):\n    print(f\"\\nğŸ”§ [{idx}/{len(search_space)}] Trying config: {config}\")\n\n    # Reset the accelerator state before creating a new Trainer\n    AcceleratorState._reset_state()\n\n    training_args = TrainingArguments(\n        output_dir=f\"./results_{config['learning_rate']}_{config['batch_size']}\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",\n        learning_rate=config[\"learning_rate\"],\n        per_device_train_batch_size=config[\"batch_size\"],\n        per_device_eval_batch_size=config[\"batch_size\"],\n        num_train_epochs=config[\"epochs\"],\n        weight_decay=0.01,\n        warmup_steps=500,\n        lr_scheduler_type=\"linear\",\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        disable_tqdm=False,\n        report_to=\"none\"\n    )\n\n    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_classes)\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        compute_metrics=compute_metrics\n    )\n\n    trainer.train()\n\n    # Predictions and report\n    preds_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:18:21.093545Z","iopub.execute_input":"2025-04-07T18:18:21.093843Z","iopub.status.idle":"2025-04-07T18:30:12.119006Z","shell.execute_reply.started":"2025-04-07T18:18:21.093818Z","shell.execute_reply":"2025-04-07T18:30:12.117932Z"}},"outputs":[{"name":"stdout","text":"\nğŸ”§ [1/3] Trying config: {'learning_rate': 2e-05, 'batch_size': 8, 'epochs': 5}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='445' max='445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [445/445 03:52, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.146400</td>\n      <td>2.120112</td>\n      <td>0.245070</td>\n      <td>0.138871</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.903700</td>\n      <td>1.815972</td>\n      <td>0.450704</td>\n      <td>0.380713</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.602300</td>\n      <td>1.492320</td>\n      <td>0.540845</td>\n      <td>0.502183</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.276300</td>\n      <td>1.236828</td>\n      <td>0.608451</td>\n      <td>0.580805</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.072400</td>\n      <td>1.077316</td>\n      <td>0.633803</td>\n      <td>0.609214</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ”§ [2/3] Trying config: {'learning_rate': 3e-05, 'batch_size': 16, 'epochs': 6}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [270/270 04:18, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.187100</td>\n      <td>2.174743</td>\n      <td>0.225352</td>\n      <td>0.158751</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.070900</td>\n      <td>2.031371</td>\n      <td>0.323944</td>\n      <td>0.266107</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.894400</td>\n      <td>1.804518</td>\n      <td>0.461972</td>\n      <td>0.404875</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.586400</td>\n      <td>1.523440</td>\n      <td>0.574648</td>\n      <td>0.543124</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.373500</td>\n      <td>1.313517</td>\n      <td>0.608451</td>\n      <td>0.573939</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.137200</td>\n      <td>1.190483</td>\n      <td>0.597183</td>\n      <td>0.570220</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ”§ [3/3] Trying config: {'learning_rate': 5e-05, 'batch_size': 16, 'epochs': 5}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 03:34, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.167900</td>\n      <td>2.132507</td>\n      <td>0.247887</td>\n      <td>0.194141</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.963800</td>\n      <td>1.922965</td>\n      <td>0.383099</td>\n      <td>0.308009</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.765900</td>\n      <td>1.624543</td>\n      <td>0.526761</td>\n      <td>0.487257</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.408800</td>\n      <td>1.297337</td>\n      <td>0.611268</td>\n      <td>0.586551</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.155400</td>\n      <td>1.181861</td>\n      <td>0.619718</td>\n      <td>0.595955</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"<h1>Qualitative Analysis</h1>","metadata":{}},{"cell_type":"code","source":"import random\n\n# Inverse transform to get string labels\ntrue_labels = label_encoder.inverse_transform(test_labels)\npredicted_labels = label_encoder.inverse_transform(preds)\n\n# Collect correct and incorrect predictions\ncorrect_samples = []\nincorrect_samples = []\n\nfor i, (true_label, predicted_label) in enumerate(zip(true_labels, predicted_labels)):\n    sample = {\n        \"text\": test_texts[i],\n        \"true\": true_label,\n        \"predicted\": predicted_label\n    }\n    if true_label == predicted_label:\n        correct_samples.append(sample)\n    else:\n        incorrect_samples.append(sample)\n\n# Show random correct predictions\nprint(\"\\nâœ… Sample Correct Predictions:\")\nfor sample in random.sample(correct_samples, min(5, len(correct_samples))):\n    print(\"\\n---\")\n    print(f\"Text: {sample['text'][:300]}\")  # Truncate long text\n    print(f\"True Label: {sample['true']}\")\n    print(f\"Predicted Label: {sample['predicted']}\")\n\n# Show random incorrect predictions\nprint(\"\\nâŒ Sample Incorrect Predictions:\")\nfor sample in random.sample(incorrect_samples, min(5, len(incorrect_samples))):\n    print(\"\\n---\")\n    print(f\"Text: {sample['text'][:300]}\")  # Truncate long text\n    print(f\"True Label: {sample['true']}\")\n    print(f\"Predicted Label: {sample['predicted']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T18:30:26.331297Z","iopub.execute_input":"2025-04-07T18:30:26.331626Z","iopub.status.idle":"2025-04-07T18:30:26.347079Z","shell.execute_reply.started":"2025-04-07T18:30:26.331599Z","shell.execute_reply":"2025-04-07T18:30:26.346399Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Sample Correct Predictions:\n\n---\nText: Multicsreen <p/> Zelo priporoÄljiv pa je velik zaslon ali veÄ zaslonov Za boljÅ¡o izrabo prostora je mogoÄe roÄno premakniti meje tako da se lahko menu skrije in forme dinamiÄno poveÄa, zmanjÅ¡a glede na to kje podatke le pregledujemo in kje jih vnaÅ¡amo. Zelo uporabna funkcionalnost Å¡e posebj za raÄun\nTrue Label: Promotion\nPredicted Label: Promotion\n\n---\nText: This is a discussion on couple problen need help within the General Maintenance, Troubleshooting & Accidents. forums, part of the Tech & Modifying & General Repairs category; hello i got a 04 impresa wrx with up pipe down pipe and reflash bough the car already modded i ... couple problen need help *\nTrue Label: Forum\nPredicted Label: Forum\n\n---\nText: Samson spared week from death 14 / 01 / 06 A DOG on death row with a taste for men in uniform has been given a reprieve . Magistrates had imposed a death sentence on three-year-old rottweiler Samson after they heard he had attacked a postman and a police officer . They were bitten in separate incide\nTrue Label: News\nPredicted Label: News\n\n---\nText: How to Find a Jobs on Craigslist Craigslist Overview: Craigslist is a good source of job listings in a specific location. Jobs are listed by location and category. You can also post your resume to your local Craigslist site. Craigslist Craigslist Job Search Options: The easiest way to find jobs on C\nTrue Label: Instruction\nPredicted Label: Instruction\n\n---\nText: I 've never been able to shed it myself , you know , I still feel very Turkish in that sense . You know , I still smell the country and it 's always so alive and real there . And I 'm also in many ways much more confident there than I am here . I 'm confident at work in a way , you know , because I \nTrue Label: Opinion/Argumentation\nPredicted Label: Opinion/Argumentation\n\nâŒ Sample Incorrect Predictions:\n\n---\nText: Do You Wash Your Hands? Washing your hands is a basic rule of hygiene. Hands are common gateways to infection. The big mistake people make is that they just rub their palms together and they don't get to the dirtiest parts of the hands -- under and around the fingernails. The key is to cover all sur\nTrue Label: News\nPredicted Label: Instruction\n\n---\nText: Dragana po spolnem odnosu doÅ¾ivlja boleÄino in krvavitev. Kateri so moÅ¾ni vzroki za to teÅ¾avo? <p/> Pred par dnevi sva imela s fantom spolni odnos. Naenkrat me je zabolelo od spodaj, Äez nekaj Äasa pa sem zaÄela kar precej krvaveti. Zadeva se je umirila Äez tri dni. Spet se je isto ponovilo Äetrti d\nTrue Label: Other\nPredicted Label: Opinion/Argumentation\n\n---\nText: Departmental Information Consequential amendments to the Migration Regulations 1994 as a Result of Shipping Reforms Commencement: 1 July 2012 Client summary From 1 July 2012, the Migration Regulations 1994 ('the Regulations') are amended to ensure that: vessels registered in the Australian Internati\nTrue Label: Legal\nPredicted Label: News\n\n---\nText: Posted by timothy on Saturday June 25 2011, @07:01PM from the oh-they-all-say-that dept. MaxBooger writes \"LulzSec, the notorious hacker group that's been on a rampage, just announced that it's disbanding . This follows 50 days' chaos during which time it took down several websites (including CIA.go\nTrue Label: Forum\nPredicted Label: Opinion/Argumentation\n\n---\nText: Romantic Places to visit Stonehenge , near Amesbury , Wiltshire The ancient stone circle of Stonehenge is one of the wonders of the world . Stone Circle Access by advanced booking only Recommended last admission 30 minutes before closing time Now a World Heritage Site , Stonehenge and all its surrou\nTrue Label: Promotion\nPredicted Label: Information/Explanation\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Replace these lists with the actual values from your training logs\ntrain_accuracy = [0.25, 0.45, 0.50, 0.56, 0.60]  # example training accuracy per epoch\nval_accuracy = [0.14, 0.38, 0.50, 0.55, 0.59]    # example validation accuracy per epoch\n\nepochs = range(1, len(train_accuracy) + 1)\n\n# Plotting\nplt.figure(figsize=(8, 6))\nplt.plot(epochs, train_accuracy, label='train', color='blue')\nplt.plot(epochs, val_accuracy, label='val', color='orange')\nplt.title('Transformer (DistilBERT) Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:16:01.197712Z","iopub.execute_input":"2025-04-07T19:16:01.198110Z","iopub.status.idle":"2025-04-07T19:16:01.796347Z","shell.execute_reply.started":"2025-04-07T19:16:01.198078Z","shell.execute_reply":"2025-04-07T19:16:01.795476Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":46}]}